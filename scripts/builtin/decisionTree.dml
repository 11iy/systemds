#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# This script implements decision trees for recoded and binned categorical and
# numerical input features. We train a single CART (classification and
# regression tree) decision trees depending on the provided labels y, either
# classification (majority vote per leaf) or regression (average per leaf).
#
# INPUT:
# ------------------------------------------------------------------------------
# X               Feature matrix in recoded/binned representation
# y               Label matrix in recoded/binned representation
# ctypes          Row-Vector of column types [1 scale/ordinal, 2 categorical]
#                 of shape 1-by-(ncol(X)+1), where the last entry is the y type
# max_depth       Maximum depth of the learned tree (stopping criterion)
# min_leaf        Minimum number of samples in leaf nodes (stopping criterion),
#                 odd number recommended to avoid 50/50 leaf label decisions
# min_split       Minimum number of samples in leaf for attempting a split
# max_features    Parameter controlling the number of features used as split
#                 candidates at tree nodes: m = ceil(num_features^max_features)
# impurity        Impurity measure: entropy, gini (default)
# seed            Fixed seed for randomization of samples and split candidates
# verbose         Flag indicating verbose debug output
# ------------------------------------------------------------------------------
#
# OUTPUT:
# ------------------------------------------------------------------------------
# M              Matrix M containing the learne trees, in linearized form
#                For example, give a feature matrix with features [a,b,c,d]
#                and the following trees, M would look as follows:
#
#                (L1)               |d<5|
#                                  /     \
#                (L2)           P1:2    |a<7|
#                                       /   \
#                (L3)                 P2:2 P3:1
#
#                --> M :=
#                [[4, 5, 0, 2, 1, 7, 0, 0, 0, 0, 0, 2, 0, 1]]
#                 |(L1)| |  (L2)   | |        (L3)         |
# ------------------------------------------------------------------------------

m_decisionTree = function(Matrix[Double] X, Matrix[Double] y, Matrix[Double] ctypes,
    Int max_depth = 10, Int min_leaf = 20, Int min_split = 50, Double max_features = 0.5,
    String impurity = "gini", Int seed = -1, Boolean verbose = FALSE)
  return(Matrix[Double] M)
{
  t1 = time();

  # validation checks
  if( max_depth > 32 )
    stop("decisionTree: invalid max_depth > 32: "+max_depth);
  if( sum(X<=0) != 0 )
    stop("decisionTree: feature matrix X is not properly recoded/binned: "+sum(X<=0));
  if( sum(y<=0) != 0 )
    stop("decisionTree: label vector y is not properly recoded/binned: "+sum(y<=0));

  # initialize input data and basic statistics
  # (we keep y2 and the indicates I in transposed form for sparsity exploitation)
  m = nrow(X); n = ncol(X);
  classify = (as.scalar(ctypes[1,n+1]) == 2);

  fdom = colMaxs(X);                  # num distinct per feature
  foffb = t(cumsum(t(fdom))) - fdom;  # feature begin
  foffe = t(cumsum(t(fdom)))          # feature end
  rix = matrix(seq(1,m)%*%matrix(1,1,n), m*n, 1)
  cix = matrix(X + foffb, m*n, 1);
  X2 = table(rix, cix, 1, m, as.scalar(foffe[,n]), FALSE); #one-hot encoded
  y2 = t(table(seq(1,m), y));
  cnt = colSums(X2);
  I = matrix(1, rows=1, cols=nrow(X));

  if( verbose ) {
    print("decisionTree: initialize with max_depth=" + max_depth + ", max_features="
      + max_features + ", impurity=" + impurity + ", seed=" + seed + ".");
    print("decisionTree: basic statistics:");
    print("-- impurity: " + computeImpurity(y2, I, impurity) );
    print("-- minFeatureCount: " + min(cnt));
    print("-- maxFeatureCount: " + max(cnt));
  }

  # queue-based node splitting
  M = matrix(0, rows=1, cols=2*(2^max_depth-1))
  queue = list(list(1,I)); # node IDs / data indicators
  maxPath = 1;
  while( length(queue) > 0 ) {
    # pop next node from queue for splitting
    [queue, node0] = remove(queue, 1);
    node = as.list(node0);
    nID = as.scalar(node[1]);
    nI = as.matrix(node[2]);
    if(verbose)
      print("decisionTree: attempting split of node "+nID+" ("+sum(nI)+" rows)");

    # find best split attribute
    nSeed = ifelse(seed==-1, seed, seed*nID);
    [f, v, IDleft, Ileft, IDright, Iright] = findBestSplit(
      X2, y2, foffb, foffe, nID, nI, min_leaf, max_features, impurity, nSeed);
    validSplit = sum(Ileft) >= min_leaf & sum(Iright) >= min_leaf;
    if(verbose)
      print("-- best split: f"+f+" <= "+v+" --> valid="+validSplit);
    if( validSplit )
      M[, 2*nID-1:2*nID] = t(as.matrix(list(f,v)));
    else
      M[, 2*nID] = computeLeafLabel(y2, nI, classify, verbose);
    maxPath = max(maxPath, floor(log(nID,2)+1));

    # split data, finalize or recurse
    if( validSplit ) {
      if( sum(Ileft) >= min_split & floor(log(IDleft,2))+2 < max_depth )
        queue = append(queue, list(IDleft,Ileft));
      else
        M[,2*IDleft] = computeLeafLabel(y2, Ileft, classify, verbose)
      if( sum(Iright) >= min_split & floor(log(IDright,2))+2 < max_depth )
        queue = append(queue, list(IDright,Iright));
      else
        M[,2*IDright] = computeLeafLabel(y2, Iright, classify, verbose)
      maxPath = max(maxPath, floor(log(IDleft,2)+1));
    }
  }

  # summary and encoding
  M = M[1, 1:2*(2^maxPath-1)];

  if(verbose) {
    print("decisionTree: final constructed tree (linearized):");
    print("--" + toString(M));
  }
}

findBestSplit = function(Matrix[Double] X2, Matrix[Double] y2, Matrix[Double] foffb, Matrix[Double] foffe,
    Int ID, Matrix[Double] I, Int min_leaf, Double max_features, String impurity, Int seed)
  return(Int f, Int v, Int IDleft, Matrix[Double] Ileft, Int IDright, Matrix[Double] Iright)
{
  # sample features iff max_features < 1
  n = ncol(foffb);
  numI = sum(I);
  feat = seq(1,n);
  if( max_features < 1.0 ) {
    rI = rand(rows=n, cols=1, seed=seed) <= (n^max_features/n);
    feat = removeEmpty(target=feat, margin="rows", select=rI);
  }

  # evaluate features and feature splits
  # (both categorical and numerical are treated similarly by
  # finding a cutoff point in the recoded/binned representation)
  R = matrix(0, rows=3, cols=nrow(feat));
  parfor( i in 1:nrow(feat) ) {
    f = as.scalar(feat[i]);
    beg = as.scalar(foffb[1,f])+1;
    end = as.scalar(foffe[1,f]);
    bestig = 0.0; bestv = -1;
    for(j in beg:end-1 ) { # lte semantics
       # construct predicate 0/1 vector
       p = table(seq(beg, j), 1, ncol(X2), 1);
       # find rows that match at least one value and appear in I
       Ileft = (t(X2 %*% p) * I) != 0;
       Iright = I * (Ileft==0);
       # compute information gain
       ig = computeImpurity(y2, I, impurity)
            - sum(Ileft)/numI * computeImpurity(y2, Ileft, impurity)
            - sum(Iright)/numI * computeImpurity(y2, Iright, impurity);
       # track best split value and index, incl validity
       if( ig > bestig & sum(Ileft) >= min_leaf & sum(Iright) >= min_leaf ) {
          bestig = ig;
          bestv = j;
       }
    }
    R[,i] = as.matrix(list(f, bestig, bestv));
  }
  ix = as.scalar(rowIndexMax(R[2,]));

  # extract indicators and IDs
  IDleft = 2 * ID;
  IDright= 2 * ID + 1;
  f = as.integer(as.scalar(feat[ix,1]));
  beg = as.scalar(foffb[1,f]);
  v = as.integer(as.scalar(R[3,ix])-beg);
  if( max(R[2,]) > 0 ) {
    p = table(seq(beg+1, beg+v), 1, ncol(X2), 1);
    Ileft = (t(X2 %*% p) * I) != 0;
    Iright = I * (Ileft==0);
  }
  else { # no information gain
    Ileft = as.matrix(0);
    Iright = as.matrix(0);
  }
}

computeImpurity = function(Matrix[Double] y2, Matrix[Double] I, String impurity)
  return(Double score)
{
  f = rowSums(y2 * I) / sum(I); # rel. freq. per category/bin
  score = 0.0;
  if( impurity == "gini" )
    score = 1 - sum(f^2); # sum(f*(1-f));
  else if( impurity == "entropy" )
    score = sum(-f * log(f));
  else
    stop("decisionTree: unsupported impurity measure: "+impurity);
}

computeLeafLabel = function(Matrix[Double] y2, Matrix[Double] I, Boolean classify, Boolean verbose)
  return(Double label)
{
  f = rowSums(y2 * I) / sum(I);
  label = ifelse(classify,
    as.scalar(rowIndexMax(t(f))), sum(f*seq(1,nrow(f))));
  if(verbose)
    print("-- leaf node label: " + label +" ("+sum(I)*max(f)+"/"+sum(I)+")");
}
